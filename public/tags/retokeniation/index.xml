<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Retokeniation on Agent Lightning ⚡ Blog</title>
    <link>http://localhost:9043/tags/retokeniation/</link>
    <description>Recent content in Retokeniation on Agent Lightning ⚡ Blog</description>
    <generator>Hugo -- 0.152.2</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Nov 2025 01:13:13 +0800</lastBuildDate>
    <atom:link href="http://localhost:9043/tags/retokeniation/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>No More Retokenization Drift</title>
      <link>http://localhost:9043/posts/no-more-retokenization-drift/</link>
      <pubDate>Tue, 18 Nov 2025 01:13:13 +0800</pubDate>
      <guid>http://localhost:9043/posts/no-more-retokenization-drift/</guid>
      <description>&lt;h1 id=&#34;no-more-retokenization-drift-returning-token-ids-via-openai-compatible-apis-matters-in-agent-rl&#34;&gt;No More Retokenization Drift: Returning Token IDs via OpenAI Compatible APIs Matters in Agent RL&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Agent Lightning (AGL) team&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Date: Oct. 2025&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR.&lt;/strong&gt; Agent often calls LLMs via OpenAI‑compatible endpoints, which previously return only string-based inputs and outputs. In &lt;strong&gt;agent RL&lt;/strong&gt;, this can lead to inconsistencies between training and inference due to the phenomenon we call &lt;strong&gt;Retokenization Drift&lt;/strong&gt;. This phenomenon occurs because tokens are detokenized during inference and subsequently retokenized during training; the two sets of tokens may differ even though their corresponding strings are identical. Now, you can ask vLLM’s OpenAI‑compatible endpoints to return the &lt;strong&gt;exact token IDs&lt;/strong&gt; for both prompts and generated responses. Pass &lt;code&gt;&amp;quot;return_token_ids&amp;quot;: true&lt;/code&gt; to &lt;code&gt;/v1/chat/completions&lt;/code&gt; or &lt;code&gt;/v1/completions&lt;/code&gt; and you’ll receive &lt;code&gt;prompt_token_ids&lt;/code&gt; and &lt;code&gt;token_ids&lt;/code&gt; alongside the regular text output. This makes &lt;strong&gt;agent RL&lt;/strong&gt; robust, as no more drift will happen. This pairs perfectly with Agent Lightning, where each model call is viewed as separate update sample without stitching; just log the returned IDs via &lt;code&gt;return_token_ids&lt;/code&gt; enabled.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
